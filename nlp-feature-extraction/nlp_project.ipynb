{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aggregate-tower",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /srv/conda/envs/notebook/lib/python3.6/site-packages (3.6.2)\n",
      "Requirement already satisfied: tqdm in /srv/conda/envs/notebook/lib/python3.6/site-packages (from nltk) (4.62.1)\n",
      "Requirement already satisfied: click in /srv/conda/envs/notebook/lib/python3.6/site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: joblib in /srv/conda/envs/notebook/lib/python3.6/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: regex in /srv/conda/envs/notebook/lib/python3.6/site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: importlib-metadata in /srv/conda/envs/notebook/lib/python3.6/site-packages (from click->nltk) (3.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from importlib-metadata->click->nltk) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /srv/conda/envs/notebook/lib/python3.6/site-packages (from importlib-metadata->click->nltk) (3.7.4.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from math import *\n",
    "import math\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "text = \"Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard.\"\n",
    "\n",
    "sentences = sent_tokenize(text) # NLTK function\n",
    "total_documents = len(sentences)\n",
    "\n",
    "def _create_frequency_matrix(sentences,stopWords):\n",
    "    frequency_matrix = {}\n",
    "    #ps = PorterStemmer()\n",
    "\n",
    "    for sent in sentences:\n",
    "        freq_table = {}\n",
    "        words = word_tokenize(sent)\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            #word = ps.stem(word)\n",
    "            if word in stopWords:\n",
    "                continue\n",
    "\n",
    "            if word in freq_table:\n",
    "                freq_table[word] += 1\n",
    "            else:\n",
    "                freq_table[word] = 1\n",
    "\n",
    "        frequency_matrix[sent[:15]] = freq_table\n",
    "\n",
    "    return frequency_matrix\n",
    "\n",
    "\n",
    "def _create_tf_matrix(freq_matrix):\n",
    "    tf_matrix = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        tf_table = {}\n",
    "\n",
    "        count_words_in_sentence = len(f_table)\n",
    "        for word, count in f_table.items():\n",
    "            tf_table[word] = count / count_words_in_sentence\n",
    "\n",
    "        tf_matrix[sent] = tf_table\n",
    "\n",
    "    return tf_matrix\n",
    "\n",
    "\n",
    "def _create_documents_per_words(freq_matrix):\n",
    "    word_per_doc_table = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        for word, count in f_table.items():\n",
    "            if word in word_per_doc_table:\n",
    "                word_per_doc_table[word] += 1\n",
    "            else:\n",
    "                word_per_doc_table[word] = 1\n",
    "\n",
    "    return word_per_doc_table\n",
    "\n",
    "\n",
    "\n",
    "def _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents):\n",
    "    idf_matrix = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        idf_table = {}\n",
    "\n",
    "        for word in f_table.keys():\n",
    "            idf_table[word] = math.log10(total_documents / float(count_doc_per_words[word]))\n",
    "\n",
    "        idf_matrix[sent] = idf_table\n",
    "\n",
    "    return idf_matrix\n",
    "\n",
    "\n",
    "def _create_tf_idf_matrix(tf_matrix, idf_matrix):\n",
    "    tf_idf_matrix = {}\n",
    "\n",
    "    for (sent1, f_table1), (sent2, f_table2) in zip(tf_matrix.items(), idf_matrix.items()):\n",
    "\n",
    "        tf_idf_table = {}\n",
    "\n",
    "        for (word1, value1), (word2, value2) in zip(f_table1.items(),\n",
    "                                                    f_table2.items()):  # here, keys are the same in both the table\n",
    "            tf_idf_table[word1] = float(value1 * value2)\n",
    "\n",
    "        tf_idf_matrix[sent1] = tf_idf_table\n",
    "\n",
    "    return tf_idf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-lighting",
   "metadata": {},
   "source": [
    "# english text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-bleeding",
   "metadata": {},
   "source": [
    "###  frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "temporal-avatar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Hello Mr. Smith': {'hello': 1, 'mr.': 1, 'smith': 1, ',': 1, 'today': 1, '?': 1}, 'The weather is ': {'weather': 1, 'great': 1, ',': 1, 'python': 1, 'awesome': 1, '.': 1}, 'The sky is pink': {'sky': 1, 'pinkish-blue': 1, '.': 1}, \"You shouldn't e\": {\"n't\": 1, 'eat': 1, 'cardboard': 1, '.': 1}}\n"
     ]
    }
   ],
   "source": [
    "stopWords = set(stopwords.words(\"english\"))\n",
    "freq=_create_frequency_matrix(sentences,stopWords)\n",
    "print(freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-assets",
   "metadata": {},
   "source": [
    "### tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "respective-assumption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Hello Mr. Smith': {'hello': 0.16666666666666666, 'mr.': 0.16666666666666666, 'smith': 0.16666666666666666, ',': 0.16666666666666666, 'today': 0.16666666666666666, '?': 0.16666666666666666}, 'The weather is ': {'weather': 0.16666666666666666, 'great': 0.16666666666666666, ',': 0.16666666666666666, 'python': 0.16666666666666666, 'awesome': 0.16666666666666666, '.': 0.16666666666666666}, 'The sky is pink': {'sky': 0.3333333333333333, 'pinkish-blue': 0.3333333333333333, '.': 0.3333333333333333}, \"You shouldn't e\": {\"n't\": 0.25, 'eat': 0.25, 'cardboard': 0.25, '.': 0.25}}\n"
     ]
    }
   ],
   "source": [
    "tf=_create_tf_matrix(freq)\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-silence",
   "metadata": {},
   "source": [
    "### how many sentences contain a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "partial-humanitarian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': 1, 'mr.': 1, 'smith': 1, ',': 2, 'today': 1, '?': 1, 'weather': 1, 'great': 1, 'python': 1, 'awesome': 1, '.': 3, 'sky': 1, 'pinkish-blue': 1, \"n't\": 1, 'eat': 1, 'cardboard': 1}\n"
     ]
    }
   ],
   "source": [
    "count_doc_per_words=_create_documents_per_words(freq)\n",
    "print(count_doc_per_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-scope",
   "metadata": {},
   "source": [
    "### idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "clean-easter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Hello Mr. Smith': {'hello': 0.6020599913279624, 'mr.': 0.6020599913279624, 'smith': 0.6020599913279624, ',': 0.3010299956639812, 'today': 0.6020599913279624, '?': 0.6020599913279624}, 'The weather is ': {'weather': 0.6020599913279624, 'great': 0.6020599913279624, ',': 0.3010299956639812, 'python': 0.6020599913279624, 'awesome': 0.6020599913279624, '.': 0.12493873660829992}, 'The sky is pink': {'sky': 0.6020599913279624, 'pinkish-blue': 0.6020599913279624, '.': 0.12493873660829992}, \"You shouldn't e\": {\"n't\": 0.6020599913279624, 'eat': 0.6020599913279624, 'cardboard': 0.6020599913279624, '.': 0.12493873660829992}}\n"
     ]
    }
   ],
   "source": [
    "idf=_create_idf_matrix(freq, count_doc_per_words, total_documents)\n",
    "print(idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-wesley",
   "metadata": {},
   "source": [
    "### tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "grateful-belfast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Hello Mr. Smith': {'hello': 0.10034333188799373, 'mr.': 0.10034333188799373, 'smith': 0.10034333188799373, ',': 0.050171665943996864, 'today': 0.10034333188799373, '?': 0.10034333188799373}, 'The weather is ': {'weather': 0.10034333188799373, 'great': 0.10034333188799373, ',': 0.050171665943996864, 'python': 0.10034333188799373, 'awesome': 0.10034333188799373, '.': 0.020823122768049984}, 'The sky is pink': {'sky': 0.20068666377598746, 'pinkish-blue': 0.20068666377598746, '.': 0.04164624553609997}, \"You shouldn't e\": {\"n't\": 0.1505149978319906, 'eat': 0.1505149978319906, 'cardboard': 0.1505149978319906, '.': 0.03123468415207498}}\n"
     ]
    }
   ],
   "source": [
    "tf_idf= _create_tf_idf_matrix(tf, idf)\n",
    "print(tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-forwarding",
   "metadata": {},
   "source": [
    "# french text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "engaging-karen",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_fr=\"bonjout monsieur lucas .j'espere que tu vas bien . aujourd'hui je vais vous presenter quelque chose tres importante.\"\n",
    "sentences = sent_tokenize(text_fr) # NLTK function\n",
    "total_documents = len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-beast",
   "metadata": {},
   "source": [
    "### frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "transsexual-equivalent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bonjout monsieu': {'bonjout': 1, 'monsieur': 1, 'lucas': 1, \".j'espere\": 1, 'vas': 1, 'bien': 1, '.': 1}, \"aujourd'hui je \": {\"aujourd'hui\": 1, 'vais': 1, 'presenter': 1, 'quelque': 1, 'chose': 1, 'tres': 1, 'importante': 1, '.': 1}}\n"
     ]
    }
   ],
   "source": [
    "stopWords = set(stopwords.words(\"french\"))\n",
    "freq=_create_frequency_matrix(sentences,stopWords)\n",
    "print(freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-milan",
   "metadata": {},
   "source": [
    "### tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "federal-cliff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bonjout monsieu': {'bonjout': 0.14285714285714285, 'monsieur': 0.14285714285714285, 'lucas': 0.14285714285714285, \".j'espere\": 0.14285714285714285, 'vas': 0.14285714285714285, 'bien': 0.14285714285714285, '.': 0.14285714285714285}, \"aujourd'hui je \": {\"aujourd'hui\": 0.125, 'vais': 0.125, 'presenter': 0.125, 'quelque': 0.125, 'chose': 0.125, 'tres': 0.125, 'importante': 0.125, '.': 0.125}}\n"
     ]
    }
   ],
   "source": [
    "tf=_create_tf_matrix(freq)\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-keeping",
   "metadata": {},
   "source": [
    "### how many sentences contain a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "controversial-funeral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bonjout': 1, 'monsieur': 1, 'lucas': 1, \".j'espere\": 1, 'vas': 1, 'bien': 1, '.': 2, \"aujourd'hui\": 1, 'vais': 1, 'presenter': 1, 'quelque': 1, 'chose': 1, 'tres': 1, 'importante': 1}\n"
     ]
    }
   ],
   "source": [
    "count_doc_per_words=_create_documents_per_words(freq)\n",
    "print(count_doc_per_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-kernel",
   "metadata": {},
   "source": [
    "### idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "empty-survey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bonjout monsieu': {'bonjout': 0.3010299956639812, 'monsieur': 0.3010299956639812, 'lucas': 0.3010299956639812, \".j'espere\": 0.3010299956639812, 'vas': 0.3010299956639812, 'bien': 0.3010299956639812, '.': 0.0}, \"aujourd'hui je \": {\"aujourd'hui\": 0.3010299956639812, 'vais': 0.3010299956639812, 'presenter': 0.3010299956639812, 'quelque': 0.3010299956639812, 'chose': 0.3010299956639812, 'tres': 0.3010299956639812, 'importante': 0.3010299956639812, '.': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "idf=_create_idf_matrix(freq, count_doc_per_words, total_documents)\n",
    "print(idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-senior",
   "metadata": {},
   "source": [
    "### tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "earlier-range",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bonjout monsieu': {'bonjout': 0.043004285094854454, 'monsieur': 0.043004285094854454, 'lucas': 0.043004285094854454, \".j'espere\": 0.043004285094854454, 'vas': 0.043004285094854454, 'bien': 0.043004285094854454, '.': 0.0}, \"aujourd'hui je \": {\"aujourd'hui\": 0.03762874945799765, 'vais': 0.03762874945799765, 'presenter': 0.03762874945799765, 'quelque': 0.03762874945799765, 'chose': 0.03762874945799765, 'tres': 0.03762874945799765, 'importante': 0.03762874945799765, '.': 0.0}}\n"
     ]
    }
   ],
   "source": [
    "tf_idf= _create_tf_idf_matrix(tf, idf)\n",
    "print(tf_idf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
